{"meta":{"title":"Morris","subtitle":"","description":"","author":"Morris","url":"https://hadoop.dpdns.org","root":"/"},"pages":[{"title":"categories","date":"2025-08-07T15:39:40.000Z","updated":"2025-08-07T15:39:40.000Z","comments":true,"path":"categories/index.html","permalink":"https://hadoop.dpdns.org/categories/index.html","excerpt":"","text":""},{"title":"repository","date":"2025-08-07T15:40:11.000Z","updated":"2025-08-07T15:40:11.000Z","comments":true,"path":"repository/index.html","permalink":"https://hadoop.dpdns.org/repository/index.html","excerpt":"","text":""},{"title":"tags","date":"2025-08-07T15:39:54.000Z","updated":"2025-08-07T15:39:54.000Z","comments":true,"path":"tags/index.html","permalink":"https://hadoop.dpdns.org/tags/index.html","excerpt":"","text":""},{"title":"books","date":"2025-08-07T15:40:24.000Z","updated":"2025-08-07T15:40:24.000Z","comments":true,"path":"books/index.html","permalink":"https://hadoop.dpdns.org/books/index.html","excerpt":"","text":""},{"title":"About ","date":"2025-08-07T15:39:08.000Z","updated":"2025-08-07T15:39:08.000Z","comments":true,"path":"about/index.html","permalink":"https://hadoop.dpdns.org/about/index.html","excerpt":"","text":"About Me嗨，我是Morris，23 岁，现居广州，正在寻找 Java 开发岗位 技能Java · Spring Boot · MyBatis · Redis · MySQL · Vue · Hadoop · Spark · Hive 我会做什么 全栈开发：Java 后端 (or Python ) + Vue 前端 数据工程：Hadoop &#x2F; Hive &#x2F; Spark &#x2F; 数据仓库 AI 落地：LangChain4j + RAG 检索增强 独立交付：从需求、编码、部署到上线 完成过的项目 广告投放系统（Spring Boot + Vue + 巨量引擎 API） AI 企业 RAG 问答系统（LangChain4j + Redis + 流式响应） 天气预警平台（Python + FastAPI + Scikit-learn） 联系方式GitHub：https://github.com/lovelxh95 email：morris@hadoop.dpdns.org Gitee：https://gitee.com/lphxhlxh"}],"posts":[{"title":"Cache替换算法","slug":"Cache替换算法","date":"2025-09-03T14:04:17.000Z","updated":"2025-09-03T14:26:20.351Z","comments":true,"path":"2025/09/03/Cache替换算法/","permalink":"https://hadoop.dpdns.org/2025/09/03/Cache%E6%9B%BF%E6%8D%A2%E7%AE%97%E6%B3%95/","excerpt":"","text":"替换算法汇总 算法 英文全称 原理 优点 缺点 适用场景 随机算法 Random Replacement 随机选择一个 Cache 块替换 实现简单，硬件开销小 未考虑访问模式，命中率低 对性能要求不高，硬件资源有限的场景 先进先出算法 FIFO（First In First Out） 替换最早进入 Cache 的块 实现简单，硬件开销小 未考虑数据访问频率，可能替换热点数据 嵌入式系统、简单处理器 最近最少使用算法 LRU（Least Recently Used） 替换最近最久未使用的块 较好地利用时间局部性，命中率高 实现较复杂，需要额外硬件支持 通用处理器、对性能要求高的场景 最不经常使用算法 LFU（Least Frequently Used） 替换访问次数最少的块 适合访问频率差异大的场景 对访问模式变化不敏感，实现复杂 特定应用场景，如数据库缓存 为什么需要 Cache 替换算法？Cache（高速缓存）是一种容量小但速度极快的存储器，位于 CPU 和主内存之间。它的作用是存放主内存中最常被访问的数据的一个副本。当 CPU 需要数据时，它首先检查 Cache。如果数据在 Cache 中（称为命中, Hit），CPU 就可以快速获取；如果不在（称为未命中, Miss），CPU 就需要从慢速的主内存中读取数据，并将其加载到 Cache 中。 由于 Cache 的容量远小于主内存，它很快就会被填满。当 Cache 已满，且 CPU 需要加载新的数据时，就必须选择一个已有的数据块（Cache Line）将其丢弃，以便为新数据腾出空间。Cache 替换算法就是用来决定“应该丢弃哪个数据块”的策略。一个好的替换算法能够尽可能保留未来最可能被用到的数据，从而提高 Cache 的命中率，提升系统整体性能。 1. 先进先出算法 (First-In, First-Out, FIFO)算法讲解FIFO 是最简单的替换算法。它将 Cache 中的数据块视为一个队列。当需要替换时，它会选择最早进入 Cache 的数据块进行替换，无论这个数据块最近是否被访问过。 示例假设我们的 Cache 有 3 个槽位（Cache Size &#x3D; 3），我们依次请求以下页面序列： 7, 0, 1, 2, 0, 3, 0, 4 请求页面 Cache 状态 (从老到新) 操作 7 [7] Miss, 7 进入 0 [7, 0] Miss, 0 进入 1 [7, 0, 1] Miss, 1 进入 (Cache 已满) 2 [0, 1, 2] Miss, 替换最早进入的7 0 [0, 1, 2] Hit 3 [1, 2, 3] Miss, 替换最早进入的0 0 [2, 3, 0] Miss, 替换最早进入的1 4 [3, 0, 4] Miss, 替换最早进入的2 在这个例子中，总共发生了 7 次 Miss 和 1 次 Hit。 优点 实现简单：只需要一个简单的队列结构即可实现，算法开销非常小。 理解容易：逻辑清晰，易于理解和调试。 缺点 性能不佳：完全不考虑数据的访问模式。一个经常被访问的热点数据，可能会因为进入 Cache 的时间比较早而被无情地替换出去，导致命中率低下。 存在 Belady 异常：在某些情况下，增加 Cache 的容量反而可能导致命中率下降。这是 FIFO 算法一个著名的缺陷。 2. 最近最少使用算法 (Least Recently Used, LRU)算法讲解LRU 算法的核心思想是“如果一个数据在最近一段时间没有被访问到，那么它在将来被访问的可能性也很小”。因此，当需要替换时，LRU 会选择最长时间没有被访问过的数据块进行替换。这个算法基于“时间局部性”原理，即最近被访问的数据很可能在不久的将来再次被访问。 示例同样，Cache 有 3 个槽位，请求序列为： 7, 0, 1, 2, 0, 3, 0, 4 请求页面 Cache 状态 (从最少使用到最近使用) 操作 7 [7] Miss, 7 进入 0 [7, 0] Miss, 0 进入 1 [7, 0, 1] Miss, 1 进入 (Cache 已满) 2 [0, 1, 2] Miss, 替换最久未使用的7 0 [1, 2, 0] Hit, 0 变为最近使用 3 [2, 0, 3] Miss, 替换最久未使用的1 0 [2, 3, 0] Hit, 0 变为最近使用 4 [3, 0, 4] Miss, 替换最久未使用的2 在这个例子中，总共发生了 6 次 Miss 和 2 次 Hit，性能优于 FIFO。 优点 性能好：充分利用了程序访问的“时间局部性”原理，在大多数场景下都有很高的命中率。 没有 Belady 异常：增加 Cache 容量总会提高或保持原有的命中率。 缺点 实现复杂，开销大：需要跟踪记录每个数据块的访问历史。常见的实现方式是使用一个双向链表和哈希表的组合，每次访问都需要更新链表节点的位置，这在硬件和软件上都有较高的开销。 3. 最不经常使用算法 (Least Frequently Used, LFU)算法讲解LFU 算法的核心思想是“如果一个数据在过去一段时间内被访问的次数很少，那么它在将来被访问的可能性也很小”。因此，当需要替换时，LFU 会选择被访问次数最少的数据块进行替换。如果存在多个访问次数相同的数据块，通常会结合 LRU 策略，替换其中最久未被访问的那个。 示例假设 Cache 有 3 个槽位，请求序列为： 1, 2, 3, 1, 2, 4, 1, 2, 5 请求页面 Cache 状态 (页面:访问次数) 操作 1 [(1:1)] Miss 2 [(1:1), (2:1)] Miss 3 [(1:1), (2:1), (3:1)] Miss (Cache 已满) 1 [(1:2), (2:1), (3:1)] Hit, 1 的计数增加 2 [(1:2), (2:2), (3:1)] Hit, 2 的计数增加 4 [(1:2), (2:2), (4:1)] Miss, 替换访问次数最少的3 1 [(1:3), (2:2), (4:1)] Hit, 1 的计数增加 2 [(1:3), (2:3), (4:1)] Hit, 2 的计数增加 5 [(1:3), (2:3), (5:1)] Miss, 替换访问次数最少的4 优点 考虑了访问频率：对于那些具有稳定访问模式、某些数据被长期频繁访问的场景，LFU 的效果可能优于 LRU。 缺点 实现更复杂：不仅要记录访问历史，还要维护每个数据块的访问计数值，通常需要使用堆或优先队列等数据结构，开销比 LRU 更大。 无法适应访问模式的变化：一个曾经被高频访问但现在不再需要的数据，可能会因为其高计数值而长时间“污染”Cache，无法被及时替换出去。 新加入的数据块容易被淘汰：一个新数据刚进入 Cache 时，其访问次数为 1，很容易在下一次替换中被立即淘汰，导致没有机会“成长”为热点数据。","categories":[{"name":"408","slug":"408","permalink":"https://hadoop.dpdns.org/categories/408/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://hadoop.dpdns.org/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}],"author":"Morris"},{"title":"Cache映射","slug":"Cache映射","date":"2025-09-03T13:44:47.000Z","updated":"2025-09-03T13:57:12.391Z","comments":true,"path":"2025/09/03/Cache映射/","permalink":"https://hadoop.dpdns.org/2025/09/03/Cache%E6%98%A0%E5%B0%84/","excerpt":"","text":"一、Cache基本概念 定义：Cache是位于CPU与主存之间的高速缓冲存储器，用于缓解CPU与主存之间的速度差异。 透明性：对程序员和编译器而言，Cache是透明的，程序员无需关心其存在与否，但理解Cache机制有助于编写高效程序。 基本术语： 主存块（Block）：主存被划分为大小相等的块。 Cache行（Line）或槽（Slot）：Cache被划分为与主存块大小相同的行或槽。 命中（Hit）：CPU访问的数据在Cache中。 缺失（Miss）：CPU访问的数据不在Cache中，需从主存调入。 二、Cache映射方式（Cache Mapping）Cache映射方式决定了主存块如何映射到Cache行中，主要有三种方式： 映射方式 特点 优点 缺点 直接映射（Direct Mapped） 每个主存块映射到Cache中固定的行 实现简单、命中时间短、无需考虑替换策略 不够灵活，易产生冲突缺失，Cache空间利用率低 全相联映射（Fully Associative） 每个主存块可映射到Cache任意行 灵活，冲突缺失为0，命中率高 实现复杂，需大量比较器，成本高，速度慢 组相联映射（Set Associative） 每组主存块映射到Cache固定组的任意行（组间模映射、组内全映射） 综合直接映射和全相联映射优点，兼顾灵活性与实现成本 实现较复杂，需一定数量比较器 三、Cache地址划分与组织结构 地址划分： 标记（Tag）：用于标识主存块。 索引（Index）：用于定位Cache行或组。 字节偏移（Byte Offset）：用于定位块内具体字节。 有效位（Valid Bit）： 标记Cache行数据是否有效（1有效，0无效或初始状态）。","categories":[{"name":"408","slug":"408","permalink":"https://hadoop.dpdns.org/categories/408/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://hadoop.dpdns.org/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}],"author":"Morris"},{"title":"Cache块有效性","slug":"Cache块有效性","date":"2025-09-03T13:10:46.000Z","updated":"2025-09-03T13:36:21.438Z","comments":true,"path":"2025/09/03/Cache块有效性/","permalink":"https://hadoop.dpdns.org/2025/09/03/Cache%E5%9D%97%E6%9C%89%E6%95%88%E6%80%A7/","excerpt":"","text":"Cache块有效性一，什么是Cache的有效性在 Cache 结构中，数据是以块（Block 或 Line）为单位存储的。每个 Cache 块通常包含以下几个部分 数据块：主存中的数据副本 标记（Tag）：用于标识对于主存地址 有效位：表示Cache块是否有效 二，为什么需要 Cache 块有效性当计算机刚启动时，Cache 为空，其中的数据是不确定的。此时，如果没有有效位，CPU 可能会误用这些无效数据，导致程序错误。因此，有效位的作用就是防止访问无效数据。 有效位 含义 0 Cache行无效，数据不可用，必须从内存重新加载 1 Cache行有效，数据可用，可以直接读取","categories":[{"name":"408","slug":"408","permalink":"https://hadoop.dpdns.org/categories/408/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://hadoop.dpdns.org/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}],"author":"Morris"},{"title":"排序算法(C++实现)","slug":"排序算法(C++实现)","date":"2025-09-01T13:34:26.000Z","updated":"2025-09-02T14:35:28.707Z","comments":true,"path":"2025/09/01/排序算法(C++实现)/","permalink":"https://hadoop.dpdns.org/2025/09/01/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95(C++%E5%AE%9E%E7%8E%B0)/","excerpt":"","text":"冒泡排序123456789void bubble_sort(int* arr, int n)&#123; for(int i=0;i&lt;n-1;i++)&#123; for(int j=0;j&lt;n-i-1;j++)&#123; if(arr[j] &gt; arr[j+1])&#123; swap(arr[j],arr[j+1]); &#125; &#125; &#125;&#125; 选择排序1234567891011void selection_sort(int* arr, int n)&#123; for(int i=0;i&lt;n-1;i++)&#123; int min_index = i; for(int j=i+1;j&lt;n;j++)&#123; if(arr[j] &lt; arr[min_index])&#123; min_index = j; &#125; &#125; swap(arr[i],arr[min_index]); &#125;&#125; 希尔排序123456789101112void shell_sort(int* arr, int n)&#123; for(int gap=n/2;gap&gt;0;gap/=2)&#123; for(int i=gap;i&lt;n;i++)&#123; int temp = arr[i]; int j; for(j=i;j&gt;=gap &amp;&amp; arr[j-gap]&gt;temp;j-=gap)&#123; arr[j] = arr[j-gap]; &#125; arr[j] = temp; &#125; &#125;&#125; 插入排序123456789101112void insertion_sort(int* arr, int n)&#123; for(int i=1;i&lt;n;i++)&#123; int key = arr[i]; int j = i-1; while(j&gt;=0 &amp;&amp; arr[j]&gt;key)&#123; arr[j+1] = arr[j]; j--; &#125; arr[j+1] = key; &#125;&#125; 快速排序123456789101112131415void quick_sort(int* arr, int low, int high)&#123; if(low &lt; high)&#123; int pivot = arr[high]; int i = low - 1; for(int j=low;j&lt;high;j++)&#123; if(arr[j] &lt; pivot)&#123; i++; swap(arr[i],arr[j]); &#125; &#125; swap(arr[i+1],arr[high]); quick_sort(arr, low, i); quick_sort(arr, i+2, high); &#125;&#125; 归并排序1234567891011121314151617181920212223242526void merge(int* arr, int left, int mid, int right)&#123; int* temp = new int[right-left+1]; int i = left, j = mid+1, k = 0; while(i &lt;= mid &amp;&amp; j &lt;= right)&#123; if(arr[i] &lt; arr[j])&#123; temp[k++] = arr[i++]; &#125;else&#123; temp[k++] = arr[j++]; &#125; &#125; while(i &lt;= mid) temp[k++] = arr[i++]; while(j &lt;= right) temp[k++] = arr[j++]; for(int i=left, k=0;i&lt;=right;i++,k++)&#123; arr[i] = temp[k]; &#125; delete[] temp;&#125;void merge_sort(int* arr, int left, int right)&#123; if(left &lt; right)&#123; int mid = (left + right) / 2; merge_sort(arr, left, mid); merge_sort(arr, mid+1, right); merge(arr, left, mid, right); &#125;&#125; main123456789101112131415161718192021222324252627282930313233343536373839int main()&#123; int n = 100000; int* arr = generate_data(n); // 评判每个算法的排序时间 clock_t start, end; start = clock(); bubble_sort(arr, n); end = clock(); cout &lt;&lt; &quot;Bubble Sort Time: &quot; &lt;&lt; (double)(end - start) / CLOCKS_PER_SEC &lt;&lt; &quot; seconds&quot; &lt;&lt; endl; start = clock(); selection_sort(arr, n); end = clock(); cout &lt;&lt; &quot;Selection Sort Time: &quot; &lt;&lt; (double)(end - start) / CLOCKS_PER_SEC &lt;&lt; &quot; seconds&quot; &lt;&lt; endl; start = clock(); shell_sort(arr, n); end = clock(); cout &lt;&lt; &quot;Shell Sort Time: &quot; &lt;&lt; (double)(end - start) / CLOCKS_PER_SEC &lt;&lt; &quot; seconds&quot; &lt;&lt; endl; start = clock(); insertion_sort(arr, n); end = clock(); cout &lt;&lt; &quot;Insertion Sort Time: &quot; &lt;&lt; (double)(end - start) / CLOCKS_PER_SEC &lt;&lt; &quot; seconds&quot; &lt;&lt; endl; start = clock(); quick_sort(arr, 0, n - 1); end = clock(); cout &lt;&lt; &quot;Quick Sort Time: &quot; &lt;&lt; (double)(end - start) / CLOCKS_PER_SEC &lt;&lt; &quot; seconds&quot; &lt;&lt; endl; start = clock(); merge_sort(arr, 0, n - 1); end = clock(); cout &lt;&lt; &quot;Merge Sort Time: &quot; &lt;&lt; (double)(end - start) / CLOCKS_PER_SEC &lt;&lt; &quot; seconds&quot; &lt;&lt; endl; delete[] arr; return 0;&#125;","categories":[{"name":"408","slug":"408","permalink":"https://hadoop.dpdns.org/categories/408/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://hadoop.dpdns.org/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"author":"Morris"},{"title":"JVM 常用命令","slug":"JVM-常用命令","date":"2025-08-11T06:18:37.000Z","updated":"2025-08-14T13:32:52.937Z","comments":true,"path":"2025/08/11/JVM-常用命令/","permalink":"https://hadoop.dpdns.org/2025/08/11/JVM-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"JPS：JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。 jstat：jstat(JVM statistics Monitoring)是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 jmap：jmap(JVM Memory Map)命令用于生成heap dump文件，如果不使用这个命令，还阔以使用-XX:+HeapDumpOnOutOfMemoryError参数来让虚拟机出现OOM的时候自动生成dump文件。 jmap不仅能生成dump文件，还阔以查询finalize执行队列、Java堆和永久代的详细信息，如当前使用率、当前使用的是哪种收集器等。 jhat： jhat(JVM Heap Analysis Tool)命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP&#x2F;HTML服务器，生成dump的分析结果后，可以在浏览器中查看。在此要注意，一般不会直接在服务器上进行分析，因为jhat是一个耗时并且耗费硬件资源的过程，一般把服务器生成的dump文件复制到本地或其他机器上进行分析。 jstack：jstack用于生成java虚拟机当前时刻的线程快照。jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。","categories":[{"name":"Java面试","slug":"Java面试","permalink":"https://hadoop.dpdns.org/categories/Java%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://hadoop.dpdns.org/tags/Java%E5%9F%BA%E7%A1%80/"}],"author":"Morris"},{"title":"sleep() 和 wait() 有什么区别","slug":"sleep-和-wait-有什么区别","date":"2025-08-11T05:54:40.000Z","updated":"2025-08-11T06:03:40.157Z","comments":true,"path":"2025/08/11/sleep-和-wait-有什么区别/","permalink":"https://hadoop.dpdns.org/2025/08/11/sleep-%E5%92%8C-wait-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","excerpt":"","text":"sleep方法：是Thread类的静态方法，当前线程将睡眠n毫秒，线程进入阻塞状态。当睡眠时间到了，会解除阻塞，进入可运行状态，等待CPU的到来。睡眠不释放锁（如果有的话） wait方法：是Object的方法，必须与synchronized关键字一起使用，线程进入阻塞状态，当notify或者notifyall被调用后，会解除阻塞。但是，只有重新占用互斥锁之后才会进入可运行状态。睡眠时，会释放互斥锁； 在 Java 中，sleep() 和 wait() 都是用于线程控制的方法，但它们有以下几个关键区别： 所属类不同 sleep() 是 Thread 类的静态方法 wait() 是 Object 类的实例方法 对锁的影响不同 sleep() 不会释放当前线程所持有的锁 wait() 会释放当前线程所持有的锁，让其他线程有机会获取该锁 唤醒方式不同 sleep() 时间到后会自动唤醒，或者被 interrupt() 方法中断唤醒 wait() 需要其他线程调用相同对象的 notify() 或 notifyAll() 方法才能唤醒，或者指定等待时间后自动唤醒 使用场景不同 sleep() 通常用于暂停线程执行一段固定时间，不依赖于其他线程的状态 wait() 通常用于线程间通信，让线程等待某个条件满足后再继续执行 调用要求不同 sleep() 可以在任何地方调用，不需要持有对象锁 wait() 必须在同步代码块（synchronized）中调用，且当前线程必须持有该对象的锁 示例代码： 123456789// sleep() 示例Thread.sleep(1000); // 让当前线程休眠1秒，不释放锁// wait() 示例synchronized (obj) &#123; obj.wait(); // 释放obj对象的锁，进入等待状态 // 或指定等待时间 obj.wait(1000); // 最多等待1秒&#125; 需要注意的是，当 wait() 被唤醒后，线程不会立即执行，而是需要重新获取对象锁后才能继续执行。","categories":[{"name":"Java面试","slug":"Java面试","permalink":"https://hadoop.dpdns.org/categories/Java%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://hadoop.dpdns.org/tags/Java%E5%9F%BA%E7%A1%80/"}],"author":"Morris"},{"title":"数据库三范式的理解与看法","slug":"数据库三范式的理解与看法","date":"2025-08-09T05:27:05.000Z","updated":"2025-08-09T05:32:28.948Z","comments":true,"path":"2025/08/09/数据库三范式的理解与看法/","permalink":"https://hadoop.dpdns.org/2025/08/09/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%89%E8%8C%83%E5%BC%8F%E7%9A%84%E7%90%86%E8%A7%A3%E4%B8%8E%E7%9C%8B%E6%B3%95/","excerpt":"","text":"数据库三范式（First Normal Form, 1NF；Second Normal Form, 2NF；Third Normal Form, 3NF）是关系型数据库设计中用于减少数据冗余、避免更新异常（插入、删除、修改异常）的核心原则。其本质是通过对数据表的结构化拆分，确保数据的“原子性”和“依赖合理性”，最终实现数据的一致性和可维护性。 1. 三范式的核心定义与作用 第一范式（1NF）：要求数据表中的每个属性（列）都是“原子性”的，即不可再分。例如，“地址”字段若包含“省、市、区”，需拆分为独立列，否则无法单独查询“某省的用户”，会导致查询效率低且数据混乱。作用：确保数据的最小粒度，为后续的依赖关系规范奠定基础。 第二范式（2NF）：在 1NF 的基础上，要求非主属性（非主键列）完全依赖于主键（而非主键的一部分，即“消除部分依赖”）。例如，“学生选课表”中，若主键是（学生 ID，课程 ID），则“学生姓名”仅依赖“学生 ID”（部分依赖），需拆分为“学生表”（存学生 ID、姓名）和“选课表”（存学生 ID、课程 ID、成绩）。作用：避免因主键部分字段变化导致的非主属性冗余（如同一学生选多门课，姓名会重复存储）。 第三范式（3NF）：在 2NF 的基础上，要求非主属性之间不存在“传递依赖”（即非主属性不能依赖于其他非主属性）。例如，“学生表”中若有“学生 ID、学院 ID、学院名称”，则“学院名称”依赖于“学院 ID”（传递依赖于主键“学生 ID”），需拆分为“学生表”（学生 ID、学院 ID）和“学院表”（学院 ID、学院名称）。作用：避免因非主属性变化导致的连锁更新（如学院名称修改时，无需更新所有学生的记录）。 2. 三范式的价值与局限性 价值：三范式是数据库设计的“基准线”，其核心价值在于通过结构化拆分减少冗余，降低数据不一致的风险。对于业务稳定、数据量中等、以“事务性操作”（如订单、用户管理）为主的系统，严格遵循三范式可显著提升数据维护效率（如修改一条学院名称，仅需改学院表，无需动学生表）。 局限性:过度规范化可能导致“拆分过度”：表数量激增，查询时需频繁进行多表连接（JOIN），反而降低查询性能（尤其数据量极大时）。例如，电商订单查询需关联“订单表、用户表、商品表、地址表、支付表”等，多表连接会增加数据库 IO 压力。此外，部分场景需“反范式设计”（主动保留冗余）：如数据仓库的报表场景，为提升查询速度，会将多表数据合并为宽表（冗余存储），牺牲部分维护性换取性能。 3. 实际应用中的平衡三范式并非“绝对准则”，而是需要结合业务场景灵活调整： 事务型系统（如银行转账）：需严格遵循三范式，优先保证数据一致性； 分析型系统（如电商数据分析）：可适当反范式，优先保证查询效率； 折中方案：通过“冗余字段 + 触发器&#x2F;定时任务”维护一致性（如订单表冗余“商品名称”，同时用触发器同步商品表的名称更新）。","categories":[{"name":"Java面试","slug":"Java面试","permalink":"https://hadoop.dpdns.org/categories/Java%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://hadoop.dpdns.org/tags/MySQL/"}],"author":"Morris"},{"title":"new String(\"abc\")创建了多少个对象？","slug":"new-String-abc-创建了多少个对象？","date":"2025-08-08T06:50:05.000Z","updated":"2025-08-08T06:57:29.236Z","comments":true,"path":"2025/08/08/new-String-abc-创建了多少个对象？/","permalink":"https://hadoop.dpdns.org/2025/08/08/new-String-abc-%E5%88%9B%E5%BB%BA%E4%BA%86%E5%A4%9A%E5%B0%91%E4%B8%AA%E5%AF%B9%E8%B1%A1%EF%BC%9F/","excerpt":"","text":"在 Java 中，使用 new String(&quot;abc&quot;) 可能创建 1 个或 2 个对象，具体取决于字符串常量池（String Pool）中是否已存在 “abc” 这个字符串： 如果常量池中不存在 “abc”： 会先在常量池中创建一个 “abc” 对象。 再通过 new 关键字在堆内存中创建一个新的 String 对象（该对象的值与常量池中的 “abc” 相同）。 总共创建 2 个对象。 如果常量池中已存在 “abc”： 仅通过 new 关键字在堆内存中创建一个新的 String 对象（该对象的值引用常量池中的 “abc”）。 总共创建 1 个对象。 这是因为 new String(&quot;abc&quot;)会强制在堆中生成一个新对象，而直接使用字符串字面量（如&quot;abc&quot;）则会优先复用常量池中的已有对象。","categories":[{"name":"Java面试","slug":"Java面试","permalink":"https://hadoop.dpdns.org/categories/Java%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://hadoop.dpdns.org/tags/Java%E5%9F%BA%E7%A1%80/"}],"author":"Morris"},{"title":"MySQL面试题","slug":"MySQL面试题","date":"2025-08-07T17:10:55.000Z","updated":"2025-08-07T17:11:33.404Z","comments":true,"path":"2025/08/08/MySQL面试题/","permalink":"https://hadoop.dpdns.org/2025/08/08/MySQL%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"MyISAM和InnoDB的区别有哪些 InnoDB支持事务, MyISAM不支持 InnoDB支持行级锁, MyISAM支持表级锁 InnoDB支持多版本并发控制(MVVC), MyISAM不支持 InnoDB支持外键, MyISAM不支持 MyISAM支持全文索引, InnoDB部分版本不支持(但可以使用Sphinx插件) MySQL怎么恢复半个月前的数据通过整库备份+binlog进行恢复. 前提是要有定期整库备份且保存了binlog日志 MySQL事务的隔离级别, 分别有什么特点 读未提交(RU): 一个事务还没提交时, 它做的变更就能被别的事务看到 读提交(RC): 一个事务提交之后, 它做的变更才会被其他事务看到 可重复读(RR): 一个事务执行过程中看到的数据, 总是跟这个事务在启动时看到的数据是一致的. 当 然在可重复读隔离级别下, 未提交变更对其他事务也是不可见的 串行化(S): 对于同一行记录, 读写都会加锁. 当出现读写锁冲突的时候, 后访问的事务必须等前一个 事务执行完成才能继续执行 MySQL中有哪几种锁 表级锁： 开销小， 加锁快； 不会出现死锁； 锁定粒度大， 发生锁冲突的概率最高， 并发度最低。 行级锁： 开销大， 加锁慢； 会出现死锁； 锁定粒度最小， 发生锁冲突的概率最低， 并发度也最 高。 页面锁： 开销和加锁时间界于表锁和行锁之间； 会出现死锁； 锁定粒度界于表锁和行锁之间， 并 发度一般。 MySQL 中有哪些不同的表格共有 5 种类型的表格： 1、MyISAM2、Heap 3、Merge 4、INNODB 5、MISAM MySQL 中InnoDB 支持的四种事务隔离级别名称，以及逐级之间的区别？ read uncommited ： 读到未提交数据 read committed： 脏读， 不可重复读 repeatable read： 可重读 serializable ： 串行事物 CHAR 和VARCHAR 的区别 char：定长，存取效率高，一般用于固定长度的表单提交数据存储，例如：身份证号，手机号，电话，密码等，长度不够的时候，会采取右补空格的方式。 varchar：不定长，更节省空间，需要用一个或者两个字节来存储数据的长度。具体规则是：如果列的最大长度小于或等于255字节，则只使用1个字节表示，否则使用2个字节。 SQL语句主要分为哪几类 数据定义语言DDL（Data Ddefinition Language）CREATE，DROP，ALTER，主要为以上操作即对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言DQL（Data Query Language）SELECT，这个较为好理解，即查询操作，以select关键字，各种简单查询，连接查询等，都属于DQL。 数据操纵语言DML（Data Manipulation Language）INSERT，UPDATE，DELETE 主要为以上操作，即对数据进行操作的，DQL与DML共同构建了常用的增删改查操作。 数据控制功能DCL（Data Control Language）GRANT，REVOKE，COMMIT，ROLLBACK主要为以上操作即对数据库安全性完整性等有操作的，可以简单的理解为权限控制等。 SQL 约束有哪些 NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 主键和外键 主键：表中经常有一个列或多列的组合，其值能唯一地标识表中的每一行，这样的一列或多列称为表的主键，通过它可强制表的实体完整性。一个表只能有一个 PRIMARY KEY 约束，而且 PRIMARY KEY 约束中的列不能接受空值。 外键：在一个表中存在的另一个表的主键称此表的外键。 in和exists的区别在 MySQL 中，IN 关键字用于在一个字段中匹配多个值 1select column from table where column in (n1, n2, ...) EXISTS 关键字用于检查子查询的结果是否为空 1select column1, column2 from table_1 where exists (select * from table2 where xxx=xxx) 如果查询的两个表大小相当，那么用in和exists差别不大。如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。 drop delete truncate的区别 Delete Truncate Drop 类型 属于DML 属于DDL 属于DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据行 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行，索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 replace into 和 insert on duplicate key update的区别 replace into和on duplcate key update都是只有在primary key或者unique key冲突的时候才会执行“更新操作”。 replace into 会将已有的数据删除然后重新插入，这样就会有一种情况，如果某些字段有默认值，但是replace into语句的字段不完整，则会设置成默认值，主键id会变更。 on duplicate key update则是执行update后面的语句 UNION ALL与UNION的区别 返回结果 union all是直接连接，取到得是所有值，记录可能有重复； union 是取唯一值，记录没有重复。 排序 union将会按照字段的顺序进行全量排序； union all只是简单的将两个结果合并后就返回。 效率 从效率上说，union all 要比union快很多，所以，如果可以确认合并的两个结果集中不包含重复数据且不需要排序时的话，那么就使用union all。","categories":[{"name":"Java面试","slug":"Java面试","permalink":"https://hadoop.dpdns.org/categories/Java%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://hadoop.dpdns.org/tags/MySQL/"}],"author":"John Doe"}],"categories":[{"name":"408","slug":"408","permalink":"https://hadoop.dpdns.org/categories/408/"},{"name":"Java面试","slug":"Java面试","permalink":"https://hadoop.dpdns.org/categories/Java%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://hadoop.dpdns.org/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://hadoop.dpdns.org/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"Java基础","slug":"Java基础","permalink":"https://hadoop.dpdns.org/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"MySQL","slug":"MySQL","permalink":"https://hadoop.dpdns.org/tags/MySQL/"}]}